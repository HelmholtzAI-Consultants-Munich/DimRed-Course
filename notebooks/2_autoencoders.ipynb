{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are neural networks that learn low-dimensional latent representations from which the original data can be reconstructed as well as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Do not run yet\n",
    " def autoencoder(X, n_components=2):        \n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(n_components,))\n",
    "        return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "    layer_sizes = [64,32,16,8]\n",
    "    #encoder\n",
    "    inputs = Input(shape=(X.shape[1],), name='encoder_input')\n",
    "    x = inputs\n",
    "    for size in layer_sizes:\n",
    "        x = Dense(size, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "\n",
    "    z_mean = Dense(n_components,kernel_initializer='he_uniform', name='latent_mean')(x)\n",
    "    z_log_var = Dense(n_components,kernel_initializer='he_uniform', name='latent_sigma')(x)\n",
    "\n",
    "    z = Lambda(sampling, output_shape=(n_components,))([z_mean, z_log_var])\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "    #decoder\n",
    "    latent_inputs = Input(shape=(n_components,), name='decoder_input_sampling')\n",
    "    x = latent_inputs\n",
    "    for size in layer_sizes[::-1]:\n",
    "        x = Dense(size, activation='relu',kernel_initializer='he_uniform')(x)\n",
    "    outputs = Dense(X.shape[1] ,activation='sigmoid',kernel_initializer='he_uniform',name='decoder_output')(x)\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        \n",
    "    #autoencoder\n",
    "    vae = Model(inputs, decoder(encoder(inputs)[2]), name='vae')\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = K.mean(K.square((x- x_decoded_mean)))\n",
    "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "    vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "    X_01 = (X-X.min())/(X.max()-X.min())\n",
    "    t0 = time()\n",
    "    if  not onlyDraw  or not 'VAE' in precomputed_results:\n",
    "        vae.fit(x=X_01,y=X_01,epochs=200,verbose=0)\n",
    "        Y_VAE = encoder.predict(X)[0]\n",
    "        viz_results['VAE'] = Y_VAE\n",
    "    else:\n",
    "        Y_VAE = viz_results['VAE']\n",
    "    t1 = time()\n",
    "    print(\"VAE: %.2g sec\" % (t1 - t0))\n",
    "    i += 1\n",
    "    ax = fig.add_subplot(n_subplots_x,n_subplots_y,i)\n",
    "    plt.scatter(Y_VAE[:, 0], Y_VAE[:, 1], c=color1, cmap=cmap)\n",
    "    plt.title(\"VAE\",fontdict = {'fontsize' : title_fontsize})\n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    plt.axis('tight')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return viz_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scArches, plain VAE or rna-seq-vae\n",
    "Can we use any pretrained model for plants on our small dataset?\n",
    "Points to mention: Avoid that samples are learned by heart â€“ use regularization; latent space should have meaningful structure (e.g. show clustering of instances)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
